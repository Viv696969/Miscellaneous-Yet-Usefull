{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486213ec-6139-4e9b-98b3-b174633042d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad939c7-505f-4b41-b269-8a5ac7b5e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=read_csv(\"data copy.csv\",names=['file','alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb5a853a-3cba-488d-b269-b9406ea177e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_1.png</td>\n",
       "      <td>ई</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_2.png</td>\n",
       "      <td>ऐ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_3.png</td>\n",
       "      <td>ओ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_4.png</td>\n",
       "      <td>औ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_5.png</td>\n",
       "      <td>अ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35515</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_440.png</td>\n",
       "      <td>ज्ञा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35516</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_441.png</td>\n",
       "      <td>ज्ञु</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35517</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_442.png</td>\n",
       "      <td>ज्ञू</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35518</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_443.png</td>\n",
       "      <td>ज्ञं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_444.png</td>\n",
       "      <td>ज्ञः</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35520 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file alpha\n",
       "0      characters-48x48\\set-1\\classic\\bold\\word_1.png     ई\n",
       "1      characters-48x48\\set-1\\classic\\bold\\word_2.png     ऐ\n",
       "2      characters-48x48\\set-1\\classic\\bold\\word_3.png     ओ\n",
       "3      characters-48x48\\set-1\\classic\\bold\\word_4.png     औ\n",
       "4      characters-48x48\\set-1\\classic\\bold\\word_5.png     अ\n",
       "...                                               ...   ...\n",
       "35515   characters-48x48\\set-4\\up\\normal\\word_440.png  ज्ञा\n",
       "35516   characters-48x48\\set-4\\up\\normal\\word_441.png  ज्ञु\n",
       "35517   characters-48x48\\set-4\\up\\normal\\word_442.png  ज्ञू\n",
       "35518   characters-48x48\\set-4\\up\\normal\\word_443.png  ज्ञं\n",
       "35519   characters-48x48\\set-4\\up\\normal\\word_444.png  ज्ञः\n",
       "\n",
       "[35520 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c388eaaf-66e4-4a8a-8834-5103398aa8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbab2ba6-3c23-4326-a915-97ae24e60504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df.file.str.split(\"\\\\\",expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9464423-8ccf-43ba-8fb7-c4894b8d65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp['label']=df['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b441eff9-b615-4f93-8292-dcef7c82c2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-1</td>\n",
       "      <td>classic</td>\n",
       "      <td>bold</td>\n",
       "      <td>word_1.png</td>\n",
       "      <td>ई</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-1</td>\n",
       "      <td>classic</td>\n",
       "      <td>bold</td>\n",
       "      <td>word_2.png</td>\n",
       "      <td>ऐ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-1</td>\n",
       "      <td>classic</td>\n",
       "      <td>bold</td>\n",
       "      <td>word_3.png</td>\n",
       "      <td>ओ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-1</td>\n",
       "      <td>classic</td>\n",
       "      <td>bold</td>\n",
       "      <td>word_4.png</td>\n",
       "      <td>औ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-1</td>\n",
       "      <td>classic</td>\n",
       "      <td>bold</td>\n",
       "      <td>word_5.png</td>\n",
       "      <td>अ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35515</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-4</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "      <td>word_440.png</td>\n",
       "      <td>ज्ञा</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35516</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-4</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "      <td>word_441.png</td>\n",
       "      <td>ज्ञु</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35517</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-4</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "      <td>word_442.png</td>\n",
       "      <td>ज्ञू</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35518</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-4</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "      <td>word_443.png</td>\n",
       "      <td>ज्ञं</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519</th>\n",
       "      <td>characters-48x48</td>\n",
       "      <td>set-4</td>\n",
       "      <td>up</td>\n",
       "      <td>normal</td>\n",
       "      <td>word_444.png</td>\n",
       "      <td>ज्ञः</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35520 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0      1        2       3             4 label\n",
       "0      characters-48x48  set-1  classic    bold    word_1.png     ई\n",
       "1      characters-48x48  set-1  classic    bold    word_2.png     ऐ\n",
       "2      characters-48x48  set-1  classic    bold    word_3.png     ओ\n",
       "3      characters-48x48  set-1  classic    bold    word_4.png     औ\n",
       "4      characters-48x48  set-1  classic    bold    word_5.png     अ\n",
       "...                 ...    ...      ...     ...           ...   ...\n",
       "35515  characters-48x48  set-4       up  normal  word_440.png  ज्ञा\n",
       "35516  characters-48x48  set-4       up  normal  word_441.png  ज्ञु\n",
       "35517  characters-48x48  set-4       up  normal  word_442.png  ज्ञू\n",
       "35518  characters-48x48  set-4       up  normal  word_443.png  ज्ञं\n",
       "35519  characters-48x48  set-4       up  normal  word_444.png  ज्ञः\n",
       "\n",
       "[35520 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd21719d-09ad-4bf0-bf1b-8c5371cf267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca51ee81-6b79-4c58-8f42-85e2b5132d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.rename(columns={0:'folder',1:'set',2:'type',3:'style',4:'name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37380951-cc46-4536-806c-ad231f792534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='set', ylabel='count'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df_temp,x='set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32b2fe5c-85a5-4a75-9e43-4142172cb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a79080b8-dcb1-4324-bc6b-7b8c82f5586d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9729b7d4-e2fe-4ccf-80f9-a23bf80e5581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='style', ylabel='count'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df_temp,x='style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ba9c9a9-c384-45c6-b774-20dd8b520e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce054e0f-c663-4b37-a9ad-32c26fb12808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1102b0e8-5911-4d9f-81f3-0a9515afd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['test']=df_temp.apply(lambda x:os.path.join(x['folder'],x['set'],x['type'],x['style'],x['name']),axis=1)\n",
    "                         # os.path.join(x.folder,x.set,x.type,x.style,x.name)\n",
    "                        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72a488ac-bd2f-4a1d-bf8d-fe3e3bc19f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['file']=df['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7eb350b5-127a-428f-94bc-3f4fd634d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('test',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33dd0811-0cbc-45ec-8bcf-c087a83fd6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['characters-48x48\\\\set-1\\\\classic\\\\bold\\\\word_1.png', 'ई'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac766094-5f69-4d54-9c88-48c886bc5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0965a389-acaa-4b69-9edc-b8c91e7a1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe28d4f-8fcf-4f13-b374-c90c6f65ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [\n",
    "    'ई','ऐ','ओ', 'औ','अ', 'आ','इ','उ', 'ऊ', 'ए', 'अं', 'अः',\n",
    "    'कि', 'की', 'के', 'कै','को', 'कौ', 'क', 'का','कु', 'कू','कं', 'कः',\n",
    "    'खि', 'खी', 'खे', 'खै','खो', 'खौ', 'ख', 'खा','खु', 'खू','खं', 'खः',\n",
    "    'गि', 'गी', 'गे', 'गै','गो', 'गौ', 'ग', 'गा','गु', 'गू','गं', 'गः',\n",
    "    'घि', 'घी', 'घे', 'घै','घो', 'घौ', 'घ', 'घा','घु', 'घू','घं', 'घः',\n",
    "    'ङि', 'ङी', 'ङे', 'ङै','ङो', 'ङौ', 'ङ', 'ङा','ङु', 'ङू','ङं', 'ङः',\n",
    "    'चि', 'ची', 'चे', 'चै','चो', 'चौ', 'च', 'चा','चु', 'चू','चं', 'चः',\n",
    "    'छि', 'छी', 'छे', 'छै','छो', 'छौ', 'छ', 'छा','छु', 'छू','छं', 'छः',\n",
    "    'जि', 'जी', 'जे', 'जै','जो', 'जौ', 'ज', 'जा','जु', 'जू','जं', 'जः',\n",
    "    'झि', 'झी', 'झे', 'झै','झो', 'झौ', 'झ', 'झा','झु', 'झू','झं', 'झः',\n",
    "    'ञि', 'ञी', 'ञे', 'ञै','ञो', 'ञौ', 'ञ', 'ञा','ञु', 'ञू','ञं', 'ञः',\n",
    "    'टि', 'टी', 'टे', 'टै','टो', 'टौ', 'ट', 'टा','टु', 'टू','टं', 'टः',\n",
    "    'ठि', 'ठी', 'ठे', 'ठै','ठो', 'ठौ', 'ठ', 'ठा','ठु', 'ठू','ठं', 'ठः',\n",
    "    'डि', 'डी', 'डे', 'डै','डो', 'डौ', 'ड', 'डा','डु', 'डू','डं', 'डः',\n",
    "    'ढि', 'ढी', 'ढे', 'ढै','ढो', 'ढौ', 'ढ', 'ढा','ढु', 'ढू','ढं', 'ढः',\n",
    "    'णि', 'णी', 'णे', 'णै','णो', 'णौ', 'ण', 'णा','णु', 'णू','णं', 'णः',\n",
    "    'ति', 'ती', 'ते', 'तै','तो', 'तौ', 'त', 'ता','तु', 'तू','तं', 'तः',\n",
    "    'थि', 'थी', 'थे', 'थै','थो', 'थौ', 'थ', 'था','थु', 'थू','थं', 'थः',\n",
    "    'दि', 'दी', 'दे', 'दै','दो', 'दौ', 'द', 'दा','दु', 'दू','दं', 'दः',\n",
    "    'धि', 'धी', 'धे', 'धै','धो', 'धौ', 'ध', 'धा','धु', 'धू','धं', 'धः',\n",
    "    'नि', 'नी', 'ने', 'नै','नो', 'नौ', 'न', 'ना','नु', 'नू','नं', 'नः',\n",
    "    'पि', 'पी', 'पे', 'पै','पो', 'पौ', 'प', 'पा','पु', 'पू','पं', 'पः',\n",
    "    'फि', 'फी', 'फे', 'फै','फो', 'फौ', 'फ', 'फा','फु', 'फू','फं', 'फः',\n",
    "    'बि', 'बी', 'बे', 'बै','बो', 'बौ', 'ब', 'बा','बु', 'बू','बं', 'बः',\n",
    "    'भि', 'भी', 'भे', 'भै','भो', 'भौ', 'भ', 'भा','भु', 'भू','भं', 'भः',\n",
    "    'मि', 'मी', 'मे', 'मै','मो', 'मौ', 'म', 'मा','मु', 'मू','मं', 'मः',\n",
    "    'यि', 'यी', 'ये', 'यै','यो', 'यौ', 'य', 'या','यु', 'यू','यं', 'यः',\n",
    "    'रि', 'री', 'रे', 'रै','रो', 'रौ', 'र', 'रा','रु', 'रू','रं', 'रः',\n",
    "    'लि', 'ली', 'ले', 'लै','लो', 'लौ', 'ल', 'ला','लु', 'लू','लं', 'लः',\n",
    "    'वि', 'वी', 'वे', 'वै','वो', 'वौ', 'व', 'वा','वु', 'वू','वं', 'वः',\n",
    "    'शि', 'शी', 'शे', 'शै','शो', 'शौ', 'श', 'शा','शु', 'शू','शं', 'शः',\n",
    "    'षि', 'षी', 'षे', 'षै','षो', 'षौ', 'ष', 'षा','षु', 'षू','षं', 'षः',\n",
    "    'सि', 'सी', 'से', 'सै','सो', 'सौ', 'स', 'सा','सु', 'सू','सं', 'सः',\n",
    "    'हि', 'ही', 'हे', 'है','हो', 'हौ', 'ह', 'हा','हु', 'हू','हं', 'हः',\n",
    "    'ळि', 'ळी', 'ळे', 'ळै','ळो', 'ळौ', 'ळ', 'ळा','ळु', 'ळू','ळं', 'ळः',\n",
    "    'क्षि', 'क्षी', 'क्षे', 'क्षै','क्षो', 'क्षौ', 'क्ष', 'क्षा','क्षु', 'क्षू','क्षं', 'क्षः',\n",
    "    'ज्ञि', 'ज्ञी', 'ज्ञे', 'ज्ञै','ज्ञो', 'ज्ञौ', 'ज्ञ', 'ज्ञा','ज्ञु', 'ज्ञू','ज्ञं', 'ज्ञः'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60f6766b-99f7-47f4-9c6e-0d341290e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_map={j:i for i,j in enumerate(alphas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6de983f-7822-48e1-8acd-32576e23b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['alpha']=df['alpha'].apply(lambda x: alpha_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82aa906c-4fba-43ac-ad63-10419cbca289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_3.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_4.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_5.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35515</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_440.png</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35516</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_441.png</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35517</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_442.png</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35518</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_443.png</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_444.png</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35520 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  alpha\n",
       "0      characters-48x48\\set-1\\classic\\bold\\word_1.png      0\n",
       "1      characters-48x48\\set-1\\classic\\bold\\word_2.png      1\n",
       "2      characters-48x48\\set-1\\classic\\bold\\word_3.png      2\n",
       "3      characters-48x48\\set-1\\classic\\bold\\word_4.png      3\n",
       "4      characters-48x48\\set-1\\classic\\bold\\word_5.png      4\n",
       "...                                               ...    ...\n",
       "35515   characters-48x48\\set-4\\up\\normal\\word_440.png    439\n",
       "35516   characters-48x48\\set-4\\up\\normal\\word_441.png    440\n",
       "35517   characters-48x48\\set-4\\up\\normal\\word_442.png    441\n",
       "35518   characters-48x48\\set-4\\up\\normal\\word_443.png    442\n",
       "35519   characters-48x48\\set-4\\up\\normal\\word_444.png    443\n",
       "\n",
       "[35520 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6c1f1199-7a49-47a2-8d06-02a792e95ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= tf.data.Dataset.from_tensor_slices((df['file'].values,df['alpha'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cbdbdeb3-8b77-478f-8011-7c8c8aa84df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80d7b0e4-d8f7-414e-86a9-8d705b12ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2500e4c9-be1c-4011-b60b-16a329e778b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(path, label):\n",
    "    # Load the image from the file path\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3)  # Decode as RGB image\n",
    "    \n",
    "    # Normalize the image to the range [0, 1]\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    # Convert the label to tf.float32\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "90b0de49-4d9c-40d6-980c-6c42df0eeafe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'decode' for 'bytes' objects doesn't apply to a 'tensorflow.python.framework.ops.EagerTensor' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,j \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m,j)\n",
      "\u001b[1;31mTypeError\u001b[0m: descriptor 'decode' for 'bytes' objects doesn't apply to a 'tensorflow.python.framework.ops.EagerTensor' object"
     ]
    }
   ],
   "source": [
    "for i,j in dataset.take(50):\n",
    "    print(bytes.decode(i),j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c1898fd9-da17-4d94-8771-388a28c4e8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file     0\n",
       "alpha    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7d5bf31f-2748-4f4c-a335-4b43bac3e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6943596a-fc12-42e3-8437-429a34a56c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ab19f63e-fddd-4a41-87b2-5889bf6d3e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_3.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_4.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_5.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35515</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_440.png</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35516</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_441.png</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35517</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_442.png</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35518</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_443.png</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_444.png</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35520 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  alpha\n",
       "0      characters-48x48\\set-1\\classic\\bold\\word_1.png      0\n",
       "1      characters-48x48\\set-1\\classic\\bold\\word_2.png      1\n",
       "2      characters-48x48\\set-1\\classic\\bold\\word_3.png      2\n",
       "3      characters-48x48\\set-1\\classic\\bold\\word_4.png      3\n",
       "4      characters-48x48\\set-1\\classic\\bold\\word_5.png      4\n",
       "...                                               ...    ...\n",
       "35515   characters-48x48\\set-4\\up\\normal\\word_440.png    439\n",
       "35516   characters-48x48\\set-4\\up\\normal\\word_441.png    440\n",
       "35517   characters-48x48\\set-4\\up\\normal\\word_442.png    441\n",
       "35518   characters-48x48\\set-4\\up\\normal\\word_443.png    442\n",
       "35519   characters-48x48\\set-4\\up\\normal\\word_444.png    443\n",
       "\n",
       "[35520 rows x 2 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8659201b-1691-4545-a40c-fe84a03fafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,data in df[32295:].iterrows():\n",
    "    img=cv.imread(data['file'])/255\n",
    "    images.append(img)\n",
    "    labels.append(data['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f56927ce-7bbd-4a8c-ad0f-d5a5302f1755",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m images\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m labels\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(labels)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:464\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    462\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall input arrays must have the same shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    466\u001b[0m result_ndim \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    467\u001b[0m axis \u001b[38;5;241m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[1;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "images=np.array(np.stack(images))\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "42ce0356-b452-4ca7-a7e4-8c9b2f3ccf82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35520, 35520)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d805f8a1-cd73-4ef5-8eb7-721e94f0740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32295</th>\n",
       "      <td>characters-48x48\\set-4\\right\\bold\\word_328.png</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32296</th>\n",
       "      <td>characters-48x48\\set-4\\right\\bold\\word_329.png</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32297</th>\n",
       "      <td>characters-48x48\\set-4\\right\\bold\\word_330.png</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32298</th>\n",
       "      <td>characters-48x48\\set-4\\right\\bold\\word_331.png</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32299</th>\n",
       "      <td>characters-48x48\\set-4\\right\\bold\\word_332.png</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35515</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_440.png</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35516</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_441.png</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35517</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_442.png</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35518</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_443.png</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_444.png</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  alpha\n",
       "32295  characters-48x48\\set-4\\right\\bold\\word_328.png    327\n",
       "32296  characters-48x48\\set-4\\right\\bold\\word_329.png    328\n",
       "32297  characters-48x48\\set-4\\right\\bold\\word_330.png    329\n",
       "32298  characters-48x48\\set-4\\right\\bold\\word_331.png    330\n",
       "32299  characters-48x48\\set-4\\right\\bold\\word_332.png    331\n",
       "...                                               ...    ...\n",
       "35515   characters-48x48\\set-4\\up\\normal\\word_440.png    439\n",
       "35516   characters-48x48\\set-4\\up\\normal\\word_441.png    440\n",
       "35517   characters-48x48\\set-4\\up\\normal\\word_442.png    441\n",
       "35518   characters-48x48\\set-4\\up\\normal\\word_443.png    442\n",
       "35519   characters-48x48\\set-4\\up\\normal\\word_444.png    443\n",
       "\n",
       "[3225 rows x 2 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[32295:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43b097f5-b2fe-4e3b-8b76-a332d5d9cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= tf.data.Dataset.from_tensor_slices((df['file'].values,df['alpha'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "895ac8ee-6417-43ae-83f1-ac477a7e55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8e80b9ff-3bdb-4417-b18c-f2b907f78182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(48, 3), dtype=float32) tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i,j in dataset.take(1):\n",
    "    print(i[0],j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2ec23941-0950-466a-987e-301a04f78535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce659b0-3749-4a96-bae6-4735cb7085b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927ce42e-8f8f-4304-991e-f23fcd18a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71bb4044-69ff-493f-aa38-da5f81e8c734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_3.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_4.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>characters-48x48\\set-1\\classic\\bold\\word_5.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35515</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_440.png</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35516</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_441.png</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35517</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_442.png</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35518</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_443.png</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35519</th>\n",
       "      <td>characters-48x48\\set-4\\up\\normal\\word_444.png</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35520 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  alpha\n",
       "0      characters-48x48\\set-1\\classic\\bold\\word_1.png      0\n",
       "1      characters-48x48\\set-1\\classic\\bold\\word_2.png      1\n",
       "2      characters-48x48\\set-1\\classic\\bold\\word_3.png      2\n",
       "3      characters-48x48\\set-1\\classic\\bold\\word_4.png      3\n",
       "4      characters-48x48\\set-1\\classic\\bold\\word_5.png      4\n",
       "...                                               ...    ...\n",
       "35515   characters-48x48\\set-4\\up\\normal\\word_440.png    439\n",
       "35516   characters-48x48\\set-4\\up\\normal\\word_441.png    440\n",
       "35517   characters-48x48\\set-4\\up\\normal\\word_442.png    441\n",
       "35518   characters-48x48\\set-4\\up\\normal\\word_443.png    442\n",
       "35519   characters-48x48\\set-4\\up\\normal\\word_444.png    443\n",
       "\n",
       "[35520 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8801a5e-150d-4077-a39a-ea374b59d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "labels=[]\n",
    "for index,data in df.iterrows():\n",
    "    img=cv.resize(cv.imread(data['file']),(48,48))/255\n",
    "    images.append(img)\n",
    "    labels.append(data['alpha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9d9c77-a59c-470a-92e0-d2fb5f952028",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=np.array(images)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab08d8b-09d5-45ef-86c8-787121d44e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35520, 48, 48, 3), (35520,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c9e68bf-1a90-4d10-93a4-23b19d9066e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(128,(3,3),input_shape=(48,48,3),activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "model.add(tf.keras.layers.Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(len(alphas),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6bbf8ea-3a92-4bc2-adff-b3ee63dab81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35e053c-e63a-47f6-b576-25c2169d1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6288649f-9763-4fcb-be12-5edb09d48822",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(images,labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bea99c1-c1a6-4f88-b5be-b197a1148c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 46, 46, 128)       3584      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 23, 23, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 21, 21, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 10, 10, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 32)          18464     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 4, 4, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 444)               14652     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145404 (567.98 KB)\n",
      "Trainable params: 145404 (567.98 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c917eb-c997-4eab-b477-00abea13a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', \n",
    "                             monitor='val_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51fddbfd-bfaf-4096-8c59-6f7d1fa2ec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 5.2937 - accuracy: 0.0379\n",
      "Epoch 1: val_accuracy improved from -inf to 0.22255, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek chouhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888/888 [==============================] - 274s 278ms/step - loss: 5.2937 - accuracy: 0.0379 - val_loss: 2.7654 - val_accuracy: 0.2226\n",
      "Epoch 2/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 1.5064 - accuracy: 0.5179\n",
      "Epoch 2: val_accuracy improved from 0.22255 to 0.70861, saving model to best_model.h5\n",
      "888/888 [==============================] - 260s 292ms/step - loss: 1.5064 - accuracy: 0.5179 - val_loss: 0.8250 - val_accuracy: 0.7086\n",
      "Epoch 3/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.4817 - accuracy: 0.8350\n",
      "Epoch 3: val_accuracy improved from 0.70861 to 0.88316, saving model to best_model.h5\n",
      "888/888 [==============================] - 293s 330ms/step - loss: 0.4817 - accuracy: 0.8350 - val_loss: 0.3425 - val_accuracy: 0.8832\n",
      "Epoch 4/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9178\n",
      "Epoch 4: val_accuracy improved from 0.88316 to 0.94538, saving model to best_model.h5\n",
      "888/888 [==============================] - 268s 302ms/step - loss: 0.2506 - accuracy: 0.9178 - val_loss: 0.1810 - val_accuracy: 0.9454\n",
      "Epoch 5/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9382\n",
      "Epoch 5: val_accuracy did not improve from 0.94538\n",
      "888/888 [==============================] - 298s 335ms/step - loss: 0.1875 - accuracy: 0.9382 - val_loss: 0.2645 - val_accuracy: 0.9119\n",
      "Epoch 6/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9497\n",
      "Epoch 6: val_accuracy improved from 0.94538 to 0.95608, saving model to best_model.h5\n",
      "888/888 [==============================] - 270s 304ms/step - loss: 0.1517 - accuracy: 0.9497 - val_loss: 0.1481 - val_accuracy: 0.9561\n",
      "Epoch 7/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9536\n",
      "Epoch 7: val_accuracy did not improve from 0.95608\n",
      "888/888 [==============================] - 338s 380ms/step - loss: 0.1403 - accuracy: 0.9536 - val_loss: 0.1373 - val_accuracy: 0.9524\n",
      "Epoch 8/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9590\n",
      "Epoch 8: val_accuracy improved from 0.95608 to 0.96678, saving model to best_model.h5\n",
      "888/888 [==============================] - 349s 393ms/step - loss: 0.1252 - accuracy: 0.9590 - val_loss: 0.0963 - val_accuracy: 0.9668\n",
      "Epoch 9/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9576\n",
      "Epoch 9: val_accuracy did not improve from 0.96678\n",
      "888/888 [==============================] - 315s 354ms/step - loss: 0.1239 - accuracy: 0.9576 - val_loss: 0.1367 - val_accuracy: 0.9548\n",
      "Epoch 10/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9655\n",
      "Epoch 10: val_accuracy did not improve from 0.96678\n",
      "888/888 [==============================] - 271s 305ms/step - loss: 0.1026 - accuracy: 0.9655 - val_loss: 0.1064 - val_accuracy: 0.9655\n",
      "Epoch 11/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9667\n",
      "Epoch 11: val_accuracy did not improve from 0.96678\n",
      "888/888 [==============================] - 233s 262ms/step - loss: 0.0952 - accuracy: 0.9667 - val_loss: 0.1041 - val_accuracy: 0.9631\n",
      "Epoch 12/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9632\n",
      "Epoch 12: val_accuracy did not improve from 0.96678\n",
      "888/888 [==============================] - 231s 260ms/step - loss: 0.1048 - accuracy: 0.9632 - val_loss: 0.1071 - val_accuracy: 0.9613\n",
      "Epoch 13/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9735\n",
      "Epoch 13: val_accuracy improved from 0.96678 to 0.97621, saving model to best_model.h5\n",
      "888/888 [==============================] - 232s 261ms/step - loss: 0.0766 - accuracy: 0.9735 - val_loss: 0.0653 - val_accuracy: 0.9762\n",
      "Epoch 14/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9688\n",
      "Epoch 14: val_accuracy did not improve from 0.97621\n",
      "888/888 [==============================] - 234s 263ms/step - loss: 0.0828 - accuracy: 0.9688 - val_loss: 0.0654 - val_accuracy: 0.9756\n",
      "Epoch 15/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9692\n",
      "Epoch 15: val_accuracy did not improve from 0.97621\n",
      "888/888 [==============================] - 233s 263ms/step - loss: 0.0840 - accuracy: 0.9692 - val_loss: 0.1289 - val_accuracy: 0.9552\n",
      "Epoch 16/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9690\n",
      "Epoch 16: val_accuracy did not improve from 0.97621\n",
      "888/888 [==============================] - 237s 267ms/step - loss: 0.0817 - accuracy: 0.9690 - val_loss: 0.1144 - val_accuracy: 0.9603\n",
      "Epoch 17/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9759\n",
      "Epoch 17: val_accuracy improved from 0.97621 to 0.97931, saving model to best_model.h5\n",
      "888/888 [==============================] - 241s 272ms/step - loss: 0.0621 - accuracy: 0.9759 - val_loss: 0.0516 - val_accuracy: 0.9793\n",
      "Epoch 18/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9676\n",
      "Epoch 18: val_accuracy did not improve from 0.97931\n",
      "888/888 [==============================] - 244s 275ms/step - loss: 0.0886 - accuracy: 0.9676 - val_loss: 0.0585 - val_accuracy: 0.9752\n",
      "Epoch 19/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9822\n",
      "Epoch 19: val_accuracy did not improve from 0.97931\n",
      "888/888 [==============================] - 261s 295ms/step - loss: 0.0436 - accuracy: 0.9822 - val_loss: 0.0720 - val_accuracy: 0.9724\n",
      "Epoch 20/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9674\n",
      "Epoch 20: val_accuracy did not improve from 0.97931\n",
      "888/888 [==============================] - 250s 282ms/step - loss: 0.0885 - accuracy: 0.9674 - val_loss: 0.0766 - val_accuracy: 0.9727\n",
      "Epoch 21/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9743\n",
      "Epoch 21: val_accuracy did not improve from 0.97931\n",
      "888/888 [==============================] - 240s 270ms/step - loss: 0.0634 - accuracy: 0.9743 - val_loss: 0.0610 - val_accuracy: 0.9764\n",
      "Epoch 22/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9774\n",
      "Epoch 22: val_accuracy did not improve from 0.97931\n",
      "888/888 [==============================] - 238s 268ms/step - loss: 0.0578 - accuracy: 0.9774 - val_loss: 0.0798 - val_accuracy: 0.9716\n",
      "Epoch 23/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9735\n",
      "Epoch 23: val_accuracy improved from 0.97931 to 0.98240, saving model to best_model.h5\n",
      "888/888 [==============================] - 236s 266ms/step - loss: 0.0702 - accuracy: 0.9735 - val_loss: 0.0381 - val_accuracy: 0.9824\n",
      "Epoch 24/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9818\n",
      "Epoch 24: val_accuracy did not improve from 0.98240\n",
      "888/888 [==============================] - 242s 272ms/step - loss: 0.0409 - accuracy: 0.9818 - val_loss: 0.0587 - val_accuracy: 0.9765\n",
      "Epoch 25/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9703\n",
      "Epoch 25: val_accuracy did not improve from 0.98240\n",
      "888/888 [==============================] - 245s 276ms/step - loss: 0.0781 - accuracy: 0.9703 - val_loss: 0.0607 - val_accuracy: 0.9756\n",
      "Epoch 26/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9779\n",
      "Epoch 26: val_accuracy did not improve from 0.98240\n",
      "888/888 [==============================] - 244s 275ms/step - loss: 0.0517 - accuracy: 0.9779 - val_loss: 0.0528 - val_accuracy: 0.9780\n",
      "Epoch 27/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9739\n",
      "Epoch 27: val_accuracy did not improve from 0.98240\n",
      "888/888 [==============================] - 240s 270ms/step - loss: 0.0637 - accuracy: 0.9739 - val_loss: 0.1191 - val_accuracy: 0.9555\n",
      "Epoch 28/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9801\n",
      "Epoch 28: val_accuracy did not improve from 0.98240\n",
      "888/888 [==============================] - 237s 267ms/step - loss: 0.0460 - accuracy: 0.9801 - val_loss: 0.0723 - val_accuracy: 0.9718\n",
      "Epoch 29/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9784\n",
      "Epoch 29: val_accuracy improved from 0.98240 to 0.98353, saving model to best_model.h5\n",
      "888/888 [==============================] - 232s 262ms/step - loss: 0.0521 - accuracy: 0.9784 - val_loss: 0.0342 - val_accuracy: 0.9835\n",
      "Epoch 30/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9850\n",
      "Epoch 30: val_accuracy improved from 0.98353 to 0.98409, saving model to best_model.h5\n",
      "888/888 [==============================] - 233s 262ms/step - loss: 0.0307 - accuracy: 0.9850 - val_loss: 0.0319 - val_accuracy: 0.9841\n",
      "Epoch 31/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9740\n",
      "Epoch 31: val_accuracy did not improve from 0.98409\n",
      "888/888 [==============================] - 232s 261ms/step - loss: 0.0697 - accuracy: 0.9740 - val_loss: 0.0647 - val_accuracy: 0.9749\n",
      "Epoch 32/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9837\n",
      "Epoch 32: val_accuracy did not improve from 0.98409\n",
      "888/888 [==============================] - 232s 261ms/step - loss: 0.0354 - accuracy: 0.9837 - val_loss: 0.0338 - val_accuracy: 0.9837\n",
      "Epoch 33/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9747\n",
      "Epoch 33: val_accuracy did not improve from 0.98409\n",
      "888/888 [==============================] - 235s 265ms/step - loss: 0.0637 - accuracy: 0.9747 - val_loss: 0.0697 - val_accuracy: 0.9727\n",
      "Epoch 34/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9801\n",
      "Epoch 34: val_accuracy did not improve from 0.98409\n",
      "888/888 [==============================] - 238s 268ms/step - loss: 0.0471 - accuracy: 0.9801 - val_loss: 0.0354 - val_accuracy: 0.9827\n",
      "Epoch 35/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9857\n",
      "Epoch 35: val_accuracy improved from 0.98409 to 0.98466, saving model to best_model.h5\n",
      "888/888 [==============================] - 233s 263ms/step - loss: 0.0281 - accuracy: 0.9857 - val_loss: 0.0320 - val_accuracy: 0.9847\n",
      "Epoch 36/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9764\n",
      "Epoch 36: val_accuracy did not improve from 0.98466\n",
      "888/888 [==============================] - 232s 261ms/step - loss: 0.0613 - accuracy: 0.9764 - val_loss: 0.0505 - val_accuracy: 0.9785\n",
      "Epoch 37/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9846\n",
      "Epoch 37: val_accuracy did not improve from 0.98466\n",
      "888/888 [==============================] - 233s 263ms/step - loss: 0.0336 - accuracy: 0.9846 - val_loss: 0.0329 - val_accuracy: 0.9834\n",
      "Epoch 38/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9863\n",
      "Epoch 38: val_accuracy did not improve from 0.98466\n",
      "888/888 [==============================] - 232s 261ms/step - loss: 0.0287 - accuracy: 0.9863 - val_loss: 0.0366 - val_accuracy: 0.9834\n",
      "Epoch 39/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9756\n",
      "Epoch 39: val_accuracy did not improve from 0.98466\n",
      "888/888 [==============================] - 233s 262ms/step - loss: 0.0644 - accuracy: 0.9756 - val_loss: 0.0365 - val_accuracy: 0.9814\n",
      "Epoch 40/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9851\n",
      "Epoch 40: val_accuracy improved from 0.98466 to 0.98550, saving model to best_model.h5\n",
      "888/888 [==============================] - 251s 283ms/step - loss: 0.0313 - accuracy: 0.9851 - val_loss: 0.0302 - val_accuracy: 0.9855\n",
      "Epoch 41/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9789\n",
      "Epoch 41: val_accuracy did not improve from 0.98550\n",
      "888/888 [==============================] - 234s 263ms/step - loss: 0.0520 - accuracy: 0.9789 - val_loss: 0.1359 - val_accuracy: 0.9486\n",
      "Epoch 42/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9838\n",
      "Epoch 42: val_accuracy improved from 0.98550 to 0.98606, saving model to best_model.h5\n",
      "888/888 [==============================] - 232s 262ms/step - loss: 0.0342 - accuracy: 0.9838 - val_loss: 0.0260 - val_accuracy: 0.9861\n",
      "Epoch 43/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9804\n",
      "Epoch 43: val_accuracy did not improve from 0.98606\n",
      "888/888 [==============================] - 232s 262ms/step - loss: 0.0455 - accuracy: 0.9804 - val_loss: 0.0601 - val_accuracy: 0.9780\n",
      "Epoch 44/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9831\n",
      "Epoch 44: val_accuracy did not improve from 0.98606\n",
      "888/888 [==============================] - 232s 261ms/step - loss: 0.0364 - accuracy: 0.9831 - val_loss: 0.0567 - val_accuracy: 0.9733\n",
      "Epoch 45/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9821\n",
      "Epoch 45: val_accuracy improved from 0.98606 to 0.98705, saving model to best_model.h5\n",
      "888/888 [==============================] - 236s 266ms/step - loss: 0.0415 - accuracy: 0.9821 - val_loss: 0.0250 - val_accuracy: 0.9870\n",
      "Epoch 46/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9877\n",
      "Epoch 46: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 233s 262ms/step - loss: 0.0233 - accuracy: 0.9877 - val_loss: 0.0345 - val_accuracy: 0.9848\n",
      "Epoch 47/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9790\n",
      "Epoch 47: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 234s 264ms/step - loss: 0.0541 - accuracy: 0.9790 - val_loss: 0.0526 - val_accuracy: 0.9804\n",
      "Epoch 48/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9872\n",
      "Epoch 48: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 233s 263ms/step - loss: 0.0259 - accuracy: 0.9872 - val_loss: 0.0248 - val_accuracy: 0.9863\n",
      "Epoch 49/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9847\n",
      "Epoch 49: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 244s 275ms/step - loss: 0.0331 - accuracy: 0.9847 - val_loss: 0.0742 - val_accuracy: 0.9731\n",
      "Epoch 50/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9792\n",
      "Epoch 50: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 235s 265ms/step - loss: 0.0519 - accuracy: 0.9792 - val_loss: 0.0334 - val_accuracy: 0.9837\n",
      "Epoch 51/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9852\n",
      "Epoch 51: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 233s 263ms/step - loss: 0.0321 - accuracy: 0.9852 - val_loss: 0.0963 - val_accuracy: 0.9649\n",
      "Epoch 52/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9845\n",
      "Epoch 52: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 234s 264ms/step - loss: 0.0355 - accuracy: 0.9845 - val_loss: 0.0309 - val_accuracy: 0.9851\n",
      "Epoch 53/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9873\n",
      "Epoch 53: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 240s 270ms/step - loss: 0.0276 - accuracy: 0.9873 - val_loss: 0.0579 - val_accuracy: 0.9758\n",
      "Epoch 54/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9817\n",
      "Epoch 54: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 234s 263ms/step - loss: 0.0454 - accuracy: 0.9817 - val_loss: 0.0577 - val_accuracy: 0.9762\n",
      "Epoch 55/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9869\n",
      "Epoch 55: val_accuracy did not improve from 0.98705\n",
      "888/888 [==============================] - 235s 264ms/step - loss: 0.0297 - accuracy: 0.9869 - val_loss: 0.0998 - val_accuracy: 0.9696\n",
      "Epoch 56/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9854 \n",
      "Epoch 56: val_accuracy improved from 0.98705 to 0.98775, saving model to best_model.h5\n",
      "888/888 [==============================] - 17705s 20s/step - loss: 0.0358 - accuracy: 0.9854 - val_loss: 0.0254 - val_accuracy: 0.9878\n",
      "Epoch 57/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9823\n",
      "Epoch 57: val_accuracy did not improve from 0.98775\n",
      "888/888 [==============================] - 262s 295ms/step - loss: 0.0443 - accuracy: 0.9823 - val_loss: 0.0343 - val_accuracy: 0.9824\n",
      "Epoch 58/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9881\n",
      "Epoch 58: val_accuracy did not improve from 0.98775\n",
      "888/888 [==============================] - 255s 287ms/step - loss: 0.0251 - accuracy: 0.9881 - val_loss: 0.0391 - val_accuracy: 0.9830\n",
      "Epoch 59/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9837\n",
      "Epoch 59: val_accuracy did not improve from 0.98775\n",
      "888/888 [==============================] - 246s 277ms/step - loss: 0.0388 - accuracy: 0.9837 - val_loss: 0.0229 - val_accuracy: 0.9869\n",
      "Epoch 60/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9895\n",
      "Epoch 60: val_accuracy improved from 0.98775 to 0.98930, saving model to best_model.h5\n",
      "888/888 [==============================] - 223s 251ms/step - loss: 0.0195 - accuracy: 0.9895 - val_loss: 0.0219 - val_accuracy: 0.9893\n",
      "Epoch 61/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9895\n",
      "Epoch 61: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 221s 249ms/step - loss: 0.0211 - accuracy: 0.9895 - val_loss: 0.0756 - val_accuracy: 0.9696\n",
      "Epoch 62/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9819\n",
      "Epoch 62: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 221s 249ms/step - loss: 0.0441 - accuracy: 0.9819 - val_loss: 0.0360 - val_accuracy: 0.9865\n",
      "Epoch 63/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9837\n",
      "Epoch 63: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 221s 249ms/step - loss: 0.0374 - accuracy: 0.9837 - val_loss: 0.0735 - val_accuracy: 0.9728\n",
      "Epoch 64/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9866\n",
      "Epoch 64: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 231s 260ms/step - loss: 0.0290 - accuracy: 0.9866 - val_loss: 0.0243 - val_accuracy: 0.9875\n",
      "Epoch 65/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9903\n",
      "Epoch 65: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 220s 248ms/step - loss: 0.0187 - accuracy: 0.9903 - val_loss: 0.0205 - val_accuracy: 0.9887\n",
      "Epoch 66/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9873\n",
      "Epoch 66: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 220s 248ms/step - loss: 0.0291 - accuracy: 0.9873 - val_loss: 0.1248 - val_accuracy: 0.9583\n",
      "Epoch 67/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9831\n",
      "Epoch 67: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 227s 256ms/step - loss: 0.0403 - accuracy: 0.9831 - val_loss: 0.0321 - val_accuracy: 0.9849\n",
      "Epoch 68/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9883\n",
      "Epoch 68: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 275s 310ms/step - loss: 0.0242 - accuracy: 0.9883 - val_loss: 0.0221 - val_accuracy: 0.9883\n",
      "Epoch 69/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9891\n",
      "Epoch 69: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 288s 325ms/step - loss: 0.0210 - accuracy: 0.9891 - val_loss: 0.0936 - val_accuracy: 0.9683\n",
      "Epoch 70/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9845\n",
      "Epoch 70: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 267s 301ms/step - loss: 0.0392 - accuracy: 0.9845 - val_loss: 0.0222 - val_accuracy: 0.9889\n",
      "Epoch 71/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9899\n",
      "Epoch 71: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 272s 307ms/step - loss: 0.0189 - accuracy: 0.9899 - val_loss: 0.0229 - val_accuracy: 0.9886\n",
      "Epoch 72/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9849\n",
      "Epoch 72: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 285s 321ms/step - loss: 0.0379 - accuracy: 0.9849 - val_loss: 0.0353 - val_accuracy: 0.9832\n",
      "Epoch 73/100\n",
      "888/888 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9895\n",
      "Epoch 73: val_accuracy did not improve from 0.98930\n",
      "888/888 [==============================] - 248s 280ms/step - loss: 0.0217 - accuracy: 0.9895 - val_loss: 0.0279 - val_accuracy: 0.9863\n",
      "Epoch 74/100\n",
      " 77/888 [=>............................] - ETA: 3:24 - loss: 0.0202 - accuracy: 0.9890"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1748\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1746\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1747\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1748\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \n\u001b[0;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    327\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m-> 1170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    293\u001b[0m         info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m info\n\u001b[1;32m--> 296\u001b[0m     \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\io_utils.py:80\u001b[0m, in \u001b[0;36mprint_msg\u001b[1;34m(message, line_break)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(message)\n\u001b[1;32m---> 80\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(message)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\iostream.py:580\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 580\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test), callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c9d8fed-e52e-412b-b014-f9aeb552f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa035699-ed20-4379-a55e-6e50c085d256",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_history.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory, f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "with open('training_history.json', 'w') as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6859c0c-d6fb-43c8-a60b-ae4a64db19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"marathi.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b1d781c-5ba4-4f59-8b2d-aea87eed2b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "history={'Train Loss':[],'Train Accuracy':[],'Val Loss':[],'Val Accuracy':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63f813c9-6b6e-4d0b-8653-4ab6ac7af65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter train loss 5.29\n",
      "Enter train acc  0.031\n",
      "Enter val loss 2.76\n",
      "Enter val acc  0.22\n",
      "Enter train loss 1.50\n",
      "Enter train acc  0.5179\n",
      "Enter val loss 0.8257\n",
      "Enter val acc  0.7086\n",
      "Enter train loss 0.4817\n",
      "Enter train acc  0.8350\n",
      "Enter val loss 0.3425\n",
      "Enter val acc  0.8832\n",
      "Enter train loss \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     t_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter train loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     t_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter train acc \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m     v_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter val loss\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    t_l=float(input(\"Enter train loss\"))\n",
    "    t_a=float(input(\"Enter train acc \"))\n",
    "    v_l=float(input(\"Enter val loss\"))\n",
    "    v_a=float(input(\"Enter val acc \"))\n",
    "    history['Train Loss'].append(t_l)\n",
    "    history['Train Accuracy'].append(t_a)\n",
    "    history['Val Loss'].append(v_l)\n",
    "    history['Val Accuracy'].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b476324-b6b8-4c32-a9af-bc881eacb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 3: val_accuracy improved from 0.70861 to 0.88316, saving model to best_model.h5\n",
    "888/888 [==============================] - 293s 330ms/step - loss: 0.4817 - accuracy: 0.8350 - val_loss: 0.3425 - val_accuracy: 0.8832\n",
    "Epoch 4/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9178\n",
    "Epoch 4: val_accuracy improved from 0.88316 to 0.94538, saving model to best_model.h5\n",
    "888/888 [==============================] - 268s 302ms/step - loss: 0.2506 - accuracy: 0.9178 - val_loss: 0.1810 - val_accuracy: 0.9454\n",
    "Epoch 5/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9382\n",
    "Epoch 5: val_accuracy did not improve from 0.94538\n",
    "888/888 [==============================] - 298s 335ms/step - loss: 0.1875 - accuracy: 0.9382 - val_loss: 0.2645 - val_accuracy: 0.9119\n",
    "Epoch 6/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9497\n",
    "Epoch 6: val_accuracy improved from 0.94538 to 0.95608, saving model to best_model.h5\n",
    "888/888 [==============================] - 270s 304ms/step - loss: 0.1517 - accuracy: 0.9497 - val_loss: 0.1481 - val_accuracy: 0.9561\n",
    "Epoch 7/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.1403 - accuracy: 0.9536\n",
    "Epoch 7: val_accuracy did not improve from 0.95608\n",
    "888/888 [==============================] - 338s 380ms/step - loss: 0.1403 - accuracy: 0.9536 - val_loss: 0.1373 - val_accuracy: 0.9524\n",
    "Epoch 8/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9590\n",
    "Epoch 8: val_accuracy improved from 0.95608 to 0.96678, saving model to best_model.h5\n",
    "888/888 [==============================] - 349s 393ms/step - loss: 0.1252 - accuracy: 0.9590 - val_loss: 0.0963 - val_accuracy: 0.9668\n",
    "Epoch 9/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9576\n",
    "Epoch 9: val_accuracy did not improve from 0.96678\n",
    "888/888 [==============================] - 315s 354ms/step - loss: 0.1239 - accuracy: 0.9576 - val_loss: 0.1367 - val_accuracy: 0.9548\n",
    "Epoch 10/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9655\n",
    "Epoch 10: val_accuracy did not improve from 0.96678\n",
    "888/888 [==============================] - 271s 305ms/step - loss: 0.1026 - accuracy: 0.9655 - val_loss: 0.1064 - val_accuracy: 0.9655\n",
    "Epoch 11/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9667\n",
    "Epoch 11: val_accuracy did not improve from 0.96678\n",
    "888/888 [==============================] - 233s 262ms/step - loss: 0.0952 - accuracy: 0.9667 - val_loss: 0.1041 - val_accuracy: 0.9631\n",
    "Epoch 12/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9632\n",
    "Epoch 12: val_accuracy did not improve from 0.96678\n",
    "888/888 [==============================] - 231s 260ms/step - loss: 0.1048 - accuracy: 0.9632 - val_loss: 0.1071 - val_accuracy: 0.9613\n",
    "Epoch 13/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9735\n",
    "Epoch 13: val_accuracy improved from 0.96678 to 0.97621, saving model to best_model.h5\n",
    "888/888 [==============================] - 232s 261ms/step - loss: 0.0766 - accuracy: 0.9735 - val_loss: 0.0653 - val_accuracy: 0.9762\n",
    "Epoch 14/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9688\n",
    "Epoch 14: val_accuracy did not improve from 0.97621\n",
    "888/888 [==============================] - 234s 263ms/step - loss: 0.0828 - accuracy: 0.9688 - val_loss: 0.0654 - val_accuracy: 0.9756\n",
    "Epoch 15/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9692\n",
    "Epoch 15: val_accuracy did not improve from 0.97621\n",
    "888/888 [==============================] - 233s 263ms/step - loss: 0.0840 - accuracy: 0.9692 - val_loss: 0.1289 - val_accuracy: 0.9552\n",
    "Epoch 16/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9690\n",
    "Epoch 16: val_accuracy did not improve from 0.97621\n",
    "888/888 [==============================] - 237s 267ms/step - loss: 0.0817 - accuracy: 0.9690 - val_loss: 0.1144 - val_accuracy: 0.9603\n",
    "Epoch 17/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9759\n",
    "Epoch 17: val_accuracy improved from 0.97621 to 0.97931, saving model to best_model.h5\n",
    "888/888 [==============================] - 241s 272ms/step - loss: 0.0621 - accuracy: 0.9759 - val_loss: 0.0516 - val_accuracy: 0.9793\n",
    "Epoch 18/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9676\n",
    "Epoch 18: val_accuracy did not improve from 0.97931\n",
    "888/888 [==============================] - 244s 275ms/step - loss: 0.0886 - accuracy: 0.9676 - val_loss: 0.0585 - val_accuracy: 0.9752\n",
    "Epoch 19/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9822\n",
    "Epoch 19: val_accuracy did not improve from 0.97931\n",
    "888/888 [==============================] - 261s 295ms/step - loss: 0.0436 - accuracy: 0.9822 - val_loss: 0.0720 - val_accuracy: 0.9724\n",
    "Epoch 20/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9674\n",
    "Epoch 20: val_accuracy did not improve from 0.97931\n",
    "888/888 [==============================] - 250s 282ms/step - loss: 0.0885 - accuracy: 0.9674 - val_loss: 0.0766 - val_accuracy: 0.9727\n",
    "Epoch 21/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9743\n",
    "Epoch 21: val_accuracy did not improve from 0.97931\n",
    "888/888 [==============================] - 240s 270ms/step - loss: 0.0634 - accuracy: 0.9743 - val_loss: 0.0610 - val_accuracy: 0.9764\n",
    "Epoch 22/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9774\n",
    "Epoch 22: val_accuracy did not improve from 0.97931\n",
    "888/888 [==============================] - 238s 268ms/step - loss: 0.0578 - accuracy: 0.9774 - val_loss: 0.0798 - val_accuracy: 0.9716\n",
    "Epoch 23/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9735\n",
    "Epoch 23: val_accuracy improved from 0.97931 to 0.98240, saving model to best_model.h5\n",
    "888/888 [==============================] - 236s 266ms/step - loss: 0.0702 - accuracy: 0.9735 - val_loss: 0.0381 - val_accuracy: 0.9824\n",
    "Epoch 24/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9818\n",
    "Epoch 24: val_accuracy did not improve from 0.98240\n",
    "888/888 [==============================] - 242s 272ms/step - loss: 0.0409 - accuracy: 0.9818 - val_loss: 0.0587 - val_accuracy: 0.9765\n",
    "Epoch 25/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9703\n",
    "Epoch 25: val_accuracy did not improve from 0.98240\n",
    "888/888 [==============================] - 245s 276ms/step - loss: 0.0781 - accuracy: 0.9703 - val_loss: 0.0607 - val_accuracy: 0.9756\n",
    "Epoch 26/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9779\n",
    "Epoch 26: val_accuracy did not improve from 0.98240\n",
    "888/888 [==============================] - 244s 275ms/step - loss: 0.0517 - accuracy: 0.9779 - val_loss: 0.0528 - val_accuracy: 0.9780\n",
    "Epoch 27/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9739\n",
    "Epoch 27: val_accuracy did not improve from 0.98240\n",
    "888/888 [==============================] - 240s 270ms/step - loss: 0.0637 - accuracy: 0.9739 - val_loss: 0.1191 - val_accuracy: 0.9555\n",
    "Epoch 28/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9801\n",
    "Epoch 28: val_accuracy did not improve from 0.98240\n",
    "888/888 [==============================] - 237s 267ms/step - loss: 0.0460 - accuracy: 0.9801 - val_loss: 0.0723 - val_accuracy: 0.9718\n",
    "Epoch 29/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9784\n",
    "Epoch 29: val_accuracy improved from 0.98240 to 0.98353, saving model to best_model.h5\n",
    "888/888 [==============================] - 232s 262ms/step - loss: 0.0521 - accuracy: 0.9784 - val_loss: 0.0342 - val_accuracy: 0.9835\n",
    "Epoch 30/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9850\n",
    "Epoch 30: val_accuracy improved from 0.98353 to 0.98409, saving model to best_model.h5\n",
    "888/888 [==============================] - 233s 262ms/step - loss: 0.0307 - accuracy: 0.9850 - val_loss: 0.0319 - val_accuracy: 0.9841\n",
    "Epoch 31/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9740\n",
    "Epoch 31: val_accuracy did not improve from 0.98409\n",
    "888/888 [==============================] - 232s 261ms/step - loss: 0.0697 - accuracy: 0.9740 - val_loss: 0.0647 - val_accuracy: 0.9749\n",
    "Epoch 32/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9837\n",
    "Epoch 32: val_accuracy did not improve from 0.98409\n",
    "888/888 [==============================] - 232s 261ms/step - loss: 0.0354 - accuracy: 0.9837 - val_loss: 0.0338 - val_accuracy: 0.9837\n",
    "Epoch 33/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9747\n",
    "Epoch 33: val_accuracy did not improve from 0.98409\n",
    "888/888 [==============================] - 235s 265ms/step - loss: 0.0637 - accuracy: 0.9747 - val_loss: 0.0697 - val_accuracy: 0.9727\n",
    "Epoch 34/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0471 - accuracy: 0.9801\n",
    "Epoch 34: val_accuracy did not improve from 0.98409\n",
    "888/888 [==============================] - 238s 268ms/step - loss: 0.0471 - accuracy: 0.9801 - val_loss: 0.0354 - val_accuracy: 0.9827\n",
    "Epoch 35/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9857\n",
    "Epoch 35: val_accuracy improved from 0.98409 to 0.98466, saving model to best_model.h5\n",
    "888/888 [==============================] - 233s 263ms/step - loss: 0.0281 - accuracy: 0.9857 - val_loss: 0.0320 - val_accuracy: 0.9847\n",
    "Epoch 36/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9764\n",
    "Epoch 36: val_accuracy did not improve from 0.98466\n",
    "888/888 [==============================] - 232s 261ms/step - loss: 0.0613 - accuracy: 0.9764 - val_loss: 0.0505 - val_accuracy: 0.9785\n",
    "Epoch 37/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9846\n",
    "Epoch 37: val_accuracy did not improve from 0.98466\n",
    "888/888 [==============================] - 233s 263ms/step - loss: 0.0336 - accuracy: 0.9846 - val_loss: 0.0329 - val_accuracy: 0.9834\n",
    "Epoch 38/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9863\n",
    "Epoch 38: val_accuracy did not improve from 0.98466\n",
    "888/888 [==============================] - 232s 261ms/step - loss: 0.0287 - accuracy: 0.9863 - val_loss: 0.0366 - val_accuracy: 0.9834\n",
    "Epoch 39/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9756\n",
    "Epoch 39: val_accuracy did not improve from 0.98466\n",
    "888/888 [==============================] - 233s 262ms/step - loss: 0.0644 - accuracy: 0.9756 - val_loss: 0.0365 - val_accuracy: 0.9814\n",
    "Epoch 40/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9851\n",
    "Epoch 40: val_accuracy improved from 0.98466 to 0.98550, saving model to best_model.h5\n",
    "888/888 [==============================] - 251s 283ms/step - loss: 0.0313 - accuracy: 0.9851 - val_loss: 0.0302 - val_accuracy: 0.9855\n",
    "Epoch 41/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9789\n",
    "Epoch 41: val_accuracy did not improve from 0.98550\n",
    "888/888 [==============================] - 234s 263ms/step - loss: 0.0520 - accuracy: 0.9789 - val_loss: 0.1359 - val_accuracy: 0.9486\n",
    "Epoch 42/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9838\n",
    "Epoch 42: val_accuracy improved from 0.98550 to 0.98606, saving model to best_model.h5\n",
    "888/888 [==============================] - 232s 262ms/step - loss: 0.0342 - accuracy: 0.9838 - val_loss: 0.0260 - val_accuracy: 0.9861\n",
    "Epoch 43/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9804\n",
    "Epoch 43: val_accuracy did not improve from 0.98606\n",
    "888/888 [==============================] - 232s 262ms/step - loss: 0.0455 - accuracy: 0.9804 - val_loss: 0.0601 - val_accuracy: 0.9780\n",
    "Epoch 44/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9831\n",
    "Epoch 44: val_accuracy did not improve from 0.98606\n",
    "888/888 [==============================] - 232s 261ms/step - loss: 0.0364 - accuracy: 0.9831 - val_loss: 0.0567 - val_accuracy: 0.9733\n",
    "Epoch 45/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9821\n",
    "Epoch 45: val_accuracy improved from 0.98606 to 0.98705, saving model to best_model.h5\n",
    "888/888 [==============================] - 236s 266ms/step - loss: 0.0415 - accuracy: 0.9821 - val_loss: 0.0250 - val_accuracy: 0.9870\n",
    "Epoch 46/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9877\n",
    "Epoch 46: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 233s 262ms/step - loss: 0.0233 - accuracy: 0.9877 - val_loss: 0.0345 - val_accuracy: 0.9848\n",
    "Epoch 47/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9790\n",
    "Epoch 47: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 234s 264ms/step - loss: 0.0541 - accuracy: 0.9790 - val_loss: 0.0526 - val_accuracy: 0.9804\n",
    "Epoch 48/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9872\n",
    "Epoch 48: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 233s 263ms/step - loss: 0.0259 - accuracy: 0.9872 - val_loss: 0.0248 - val_accuracy: 0.9863\n",
    "Epoch 49/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9847\n",
    "Epoch 49: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 244s 275ms/step - loss: 0.0331 - accuracy: 0.9847 - val_loss: 0.0742 - val_accuracy: 0.9731\n",
    "Epoch 50/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9792\n",
    "Epoch 50: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 235s 265ms/step - loss: 0.0519 - accuracy: 0.9792 - val_loss: 0.0334 - val_accuracy: 0.9837\n",
    "Epoch 51/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9852\n",
    "Epoch 51: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 233s 263ms/step - loss: 0.0321 - accuracy: 0.9852 - val_loss: 0.0963 - val_accuracy: 0.9649\n",
    "Epoch 52/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9845\n",
    "Epoch 52: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 234s 264ms/step - loss: 0.0355 - accuracy: 0.9845 - val_loss: 0.0309 - val_accuracy: 0.9851\n",
    "Epoch 53/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9873\n",
    "Epoch 53: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 240s 270ms/step - loss: 0.0276 - accuracy: 0.9873 - val_loss: 0.0579 - val_accuracy: 0.9758\n",
    "Epoch 54/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9817\n",
    "Epoch 54: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 234s 263ms/step - loss: 0.0454 - accuracy: 0.9817 - val_loss: 0.0577 - val_accuracy: 0.9762\n",
    "Epoch 55/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9869\n",
    "Epoch 55: val_accuracy did not improve from 0.98705\n",
    "888/888 [==============================] - 235s 264ms/step - loss: 0.0297 - accuracy: 0.9869 - val_loss: 0.0998 - val_accuracy: 0.9696\n",
    "Epoch 56/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9854 \n",
    "Epoch 56: val_accuracy improved from 0.98705 to 0.98775, saving model to best_model.h5\n",
    "888/888 [==============================] - 17705s 20s/step - loss: 0.0358 - accuracy: 0.9854 - val_loss: 0.0254 - val_accuracy: 0.9878\n",
    "Epoch 57/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9823\n",
    "Epoch 57: val_accuracy did not improve from 0.98775\n",
    "888/888 [==============================] - 262s 295ms/step - loss: 0.0443 - accuracy: 0.9823 - val_loss: 0.0343 - val_accuracy: 0.9824\n",
    "Epoch 58/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9881\n",
    "Epoch 58: val_accuracy did not improve from 0.98775\n",
    "888/888 [==============================] - 255s 287ms/step - loss: 0.0251 - accuracy: 0.9881 - val_loss: 0.0391 - val_accuracy: 0.9830\n",
    "Epoch 59/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9837\n",
    "Epoch 59: val_accuracy did not improve from 0.98775\n",
    "888/888 [==============================] - 246s 277ms/step - loss: 0.0388 - accuracy: 0.9837 - val_loss: 0.0229 - val_accuracy: 0.9869\n",
    "Epoch 60/100\n",
    "888/888 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9895\n",
    "Epoch 60: val_accuracy improved from 0.98775 to 0.98930, saving model to best_model.h5\n",
    "888/888 [==============================] - 223s 251ms/step - loss: 0.0195 - accuracy: 0.9895 - val_loss: 0.0219 - val_accuracy: 0.9893"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
